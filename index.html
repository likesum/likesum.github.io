<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Zhihao (Zach) Xia</title>

  <meta name="author" content="Zhihao (Zach) Xia">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-VC2W5TNBXK"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-VC2W5TNBXK');
  </script>
</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Zhihao (Zach) Xia</name>
                  </p>
                  <p>I am a research scientist on <a href="http://graphics.stanford.edu/~levoy/">Marc Levoy</a>'s <a
                      href="https://www.theverge.com/2020/7/20/21331331/google-pixel-camera-app-lead-adobe-marc-levoy">computational
                      photography team</a> at <a href="https://www.adobe.com/">Adobe</a>, where I work on computer
                    vision, computational photography and machine learning.
                  </p>
                  <p>I received my PhD from <a href="https://wustl.edu/">WashU</a> advised by <a
                      href="https://projects.ayanc.org/">Ayan Chakrabarti</a>. Prior to WashU, I got my Bachelors from
                    <a href="http://en.scgy.ustc.edu.cn/">School of the Gifted Young</a> at <a
                      href="http://en.ustc.edu.cn/">USTC</a>. I've also spent time at research labs at <a
                      href="https://research.google/">Google Research</a>, <a href="https://research.adobe.com/">Adobe
                      Research</a>, <a href="http://www.nus.edu.sg/">NUS</a> and <a
                      href="https://www.kaust.edu.sa/">KAUST</a>.
                  </p>
                  <p><strong><span style="font-size: 15px">Internships:</span></strong> I'm always happy to host
                    research interns at Adobe. If you are interested in interning with me, please send me an
                    email describing your past experience and current research interests.
                  </p>
                  <p align=center>
                    <a href="mailto:zxia@ad**e.com">Email</a> &nbsp/&nbsp
                    <a href="https://likesum.github.io/cv/cv.pdf">CV</a> &nbsp/&nbsp
                    <a href="https://scholar.google.com/citations?hl=en&user=Rc4ZMCEAAAAJ">Google Scholar</a>&nbsp/&nbsp
                    <a href="https://github.com/likesum">GitHub</a>
                  </p>
                </td>
                <td width="33%">
                  <img src="icon_circle.gif">
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Research</heading>
                  <p>
                    My research interests include computer vision, computational photography and deep learning. I am
                    particularly interested in the design of accurate and efficient algorithms for visual inference ---
                    reasoning different aspects of visual appearance (geometry, light, colors, etc) from images and
                    videos.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <!-- fixup -->
              <tr onmouseout="fixup_stop()" onmouseover="fixup_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='fixup_image'><img src='fixup/after.jpg' width="100%"></div>
                    <img src='fixup/before.jpg' width="100%">
                  </div>
                  <script type="text/javascript">
                    function fixup_start() {
                      document.getElementById('fixup_image').style.opacity = "1";
                    }

                    function fixup_stop() {
                      document.getElementById('fixup_image').style.opacity = "0";
                    }
                    fixup_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://time-reversal.github.io/">
                    <papertitle>Magic Fixup: Streamlining Photo Editing by Watching Dynamic Videos</papertitle>
                  </a>
                  <br>
                  <a href="https://hadizayer.github.io/">Hadi AlZayer</a>,
                  <strong><span style="font-size: 14px">Zhihao Xia</span></strong>,
                  <a href="https://ceciliavision.github.io/">Xuaner Zhang</a>,
                  <a href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a>,
                  <a href="https://jbhuang0604.github.io/">Jia-Bin Huang</a>,
                  <a href="http://mgharbi.com/">Michaël Gharbi</a>
                  <br>
                  <em>Arxiv</em>, 2024 <br>
                  <a href="https://magic-fixup.github.io/">project page</a>/
                  <a href="https://arxiv.org/abs/2403.13044">paper</a>/
                  <a href="https://github.com/adobe-research/MagicFixup">code</a>
                  <p></p>
                  <p>A generative model that, given a coarsely edited image, synthesizes a photorealistic output that
                    follows the prescribed layout.
                  </p>
                </td>
              </tr>

              <!-- inbetween -->
              <tr onmouseout="inbetween_stop()" onmouseover="inbetween_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='inbetween_video'>
                      <video width=100% height=100% muted loop playsinline>
                        <source src="inbetween/after.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video>
                    </div>
                  </div>
                  <script type="text/javascript">
                    function inbetween_start() {
                      document.getElementById('inbetween_video').getElementsByTagName('video')[0].play();
                    }

                    function inbetween_stop() {
                      document.getElementById('inbetween_video').getElementsByTagName('video')[0].pause();
                    }
                    inbetween_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://time-reversal.github.io/">
                    <papertitle>Explorative Inbetweening of Time and Space</papertitle>
                  </a>
                  <br>
                  <a href="https://ps.is.mpg.de/person/hfeng">Haiwen Feng</a>,
                  Zheng Ding,
                  <strong><span style="font-size: 14px">Zhihao Xia</span></strong>,
                  <a href="https://sniklaus.com/">Simon Niklaus</a>,
                  <a href="https://is.mpg.de/~vabrevaya">Victoria Abrevaya</a>,
                  <a href="https://ps.is.mpg.de/person/black">Michael J. Black</a>,
                  <a href="https://ceciliavision.github.io/">Xuaner Zhang</a>
                  <br>
                  <em>ECCV</em>, 2024 <br>
                  <a href="https://time-reversal.github.io/">project page</a>/
                  <a href="https://arxiv.org/pdf/2312.17161.pdf">paper</a>/
                  <a href="https://github.com/HavenFeng/time_reversal">code</a>
                  <p></p>
                  <p>Control pre-trained video generation models to synthesize arbitrary camera and subject motion based
                    only on a given start and end frame.
                  </p>
                </td>
              </tr>

              <!-- gen2res -->
              <tr onmouseout="gen2res_stop()" onmouseover="gen2res_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='gen2res_image'>
                      <video width=100% height=100% muted autoplay loop onloadedmetadata="this.playbackRate = 0.5;">
                        <source src="gen2res/after.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video>
                    </div>
                    <img src='gen2res/before.jpg' width="160">
                  </div>
                  <script type="text/javascript">
                    function gen2res_start() {
                      document.getElementById('gen2res_image').style.opacity = "1";
                    }

                    function gen2res_stop() {
                      document.getElementById('gen2res_image').style.opacity = "0";
                    }
                    gen2res_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://gen2res.github.io/">
                    <papertitle>Restoration by Generation with Constrained Priors</papertitle>
                  </a>
                  <br>
                  Zheng Ding,
                  <a href="https://ceciliavision.github.io/">Xuaner Zhang</a>,
                  <a href="https://pages.ucsd.edu/~ztu">Zhuowen Tu</a>,
                  <strong><span style="font-size: 14px">Zhihao Xia</span></strong>
                  <br>
                  <em>CVPR</em>, 2024 <br>
                  <a href="https://gen2res.github.io/">project page</a>/
                  <a href="https://arxiv.org/pdf/2312.17161.pdf">paper</a>/
                  <a href="https://github.com/adobe-research/gen2res">code</a>
                  <p></p>
                  <p>A method to directly apply a pre-trained diffusion model to blind image restoration, by
                    constraining the generative space using a set of anchor images.
                  </p>
                </td>
              </tr>

              <!-- burstsr -->
              <tr onmouseout="burstsr_stop()" onmouseover="burstsr_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='burstsr_image'><img src='burstsr/after.jpg' width="100%"></div>
                    <img src='burstsr/before.jpg' width="100%">
                  </div>
                  <script type="text/javascript">
                    function burstsr_start() {
                      document.getElementById('burstsr_image').style.opacity = "1";
                    }

                    function burstsr_stop() {
                      document.getElementById('burstsr_image').style.opacity = "0";
                    }
                    burstsr_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a
                    href="https://openaccess.thecvf.com/content/ICCV2023/html/Bhat_Self-Supervised_Burst_Super-Resolution_ICCV_2023_paper.html">
                    <papertitle>Self-Supervised Burst Super-Resolution</papertitle>
                  </a>
                  <br>
                  <a href="https://goutamgmb.github.io/">Goutam Bhat</a>,
                  <a href="http://mgharbi.com/">Michaël Gharbi</a>,
                  <a href="https://people.csail.mit.edu/jiawen/">Jiawen Chen</a>,
                  <a
                    href="https://ee.ethz.ch/the-department/faculty/professors/person-detail.OTAyMzM=.TGlzdC80MTEsMTA1ODA0MjU5.html">Luc
                    Van Gool</a>,
                  <strong><span style="font-size: 14px">Zhihao Xia</span></strong>
                  <br>
                  <em>ICCV</em>, 2023 <br>
                  <a
                    href="https://openaccess.thecvf.com/content/ICCV2023/papers/Bhat_Self-Supervised_Burst_Super-Resolution_ICCV_2023_paper.pdf">paper</a>
                  <p></p>
                  <p>A self-supervised training strategy for burst super-resolution that only uses noisy low-resolution
                    bursts during training.
                  </p>
                </td>
              </tr>

              <!-- diffrig -->
              <tr onmouseout="diffrig_stop()" onmouseover="diffrig_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='diffrig_image'><video width=100% height=100% muted autoplay loop>
                        <source src="diffrig/after.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <img src='diffrig/before.jpg' width="160">
                  </div>
                  <script type="text/javascript">
                    function diffrig_start() {
                      document.getElementById('diffrig_image').style.opacity = "1";
                    }

                    function diffrig_stop() {
                      document.getElementById('diffrig_image').style.opacity = "0";
                    }
                    diffrig_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://diffusionrig.github.io/">
                    <papertitle>DiffusionRig: Learning Personalized Priors for Facial Appearance Editing</papertitle>
                  </a>
                  <br>
                  Zheng Ding,
                  <a href="https://ceciliavision.github.io/">Xuaner Zhang</a>,
                  <strong><span style="font-size: 14px">Zhihao Xia</span></strong>,
                  <a href="https://lcjebe.github.io/">Lars Jebe</a>,
                  <a href="https://pages.ucsd.edu/~ztu">Zhuowen Tu</a>,
                  <a href="https://xiuming.info/">Xiuming Zhang</a>
                  <br>
                  <em>CVPR</em>, 2023 <br>
                  <a href="https://diffusionrig.github.io/">project page</a> /
                  <a href="https://arxiv.org/pdf/2304.06711.pdf">paper</a> /
                  <a href="https://www.youtube.com/watch?v=6ZQbiNiJJEE">video</a> /
                  <a href="https://github.com/adobe-research/diffusion-rig">code</a>
                  <p></p>
                  <p>A custom diffusion model that can "rig" the lighting, facial expression, and head pose in a
                    portrait photo while maintaining identity and preserving high-frequency features.
                  </p>
                </td>
              </tr>

              <!-- paraharm -->
              <tr onmouseout="paraharm_stop()" onmouseover="paraharm_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='paraharm_image'><img src='paraharm/after.jpg' width="100%"></div>
                    <img src='paraharm/before.jpg' width="100%">
                  </div>
                  <script type="text/javascript">
                    function paraharm_start() {
                      document.getElementById('paraharm_image').style.opacity = "1";
                    }
                    function paraharm_stop() {
                      document.getElementById('paraharm_image').style.opacity = "0";
                    }
                    paraharm_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="http://people.eecs.berkeley.edu/~kewang/sprih/">
                    <papertitle>Semi-supervised Parametric Real-world Image Harmonization</papertitle>
                  </a>
                  <br>
                  <a href="https://people.eecs.berkeley.edu/~kewang/">Ke Wang</a>,
                  <a href="http://mgharbi.com/">Michaël Gharbi</a>,
                  <a href="https://sites.google.com/site/hezhangsprinter/">He Zhang</a>,
                  <strong><span style="font-size: 14px">Zhihao Xia</span></strong>,
                  <a href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a>
                  <br>
                  <em>CVPR</em>, 2023 <br>
                  <a href="http://people.eecs.berkeley.edu/~kewang/sprih/">project page</a> /
                  <a href="https://arxiv.org/abs/2303.00157">paper</a>
                  <!-- <a href="">code</a> -->
                  <p></p>
                  <p>A parametric harmonization method employing semi-supervised training to learn local
                    appearance harmonization from unpaired real composites.
                  </p>
                </td>
              </tr>

              <!-- neuralisp -->
              <tr onmouseout="neuralisp_stop()" onmouseover="neuralisp_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='neuralisp_image'><img src='neural_isp/before.jpg' width="100%"></div>
                    <img src='neural_isp/before.jpg' width="100%">
                  </div>
                  <script type="text/javascript">
                    function neuralisp_start() {
                      document.getElementById('neuralisp_image').style.opacity = "1";
                    }
                    function neuralisp_stop() {
                      document.getElementById('neuralisp_image').style.opacity = "0";
                    }
                    neuralisp_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://light.princeton.edu/publication/neural-photo-finishing/">
                    <papertitle>Neural Photo-Finishing</papertitle>
                  </a>
                  <br>
                  <a href="https://ethan-tseng.github.io/">Ethan Tseng</a>,
                  <a href="https://www.alexyuxuanzhang.com/">Yuxuan Zhang</a>,
                  <a href="https://lcjebe.github.io/">Lars Jebe</a>,
                  <a href="https://ceciliavision.github.io/">Xuaner Zhang</a>,
                  <strong><span style="font-size: 14px">Zhihao Xia</span></strong>,
                  Yifei Fan,
                  <br>
                  <a href="https://www.cs.princeton.edu/~fheide/">Felix Heide</a>*,
                  <a href="https://people.csail.mit.edu/jiawen/">Jiawen Chen</a>*
                  <br>
                  <em>SIGGRAPH Asia</em>, 2022 <br>
                  <a href="https://light.princeton.edu/publication/neural-photo-finishing/">project page</a> /
                  <a href="https://openreview.net/pdf?id=a60XE_dEnuO">paper</a> /
                  <a
                    href="https://light.cs.princeton.edu/wp-content/uploads/2022/11/Neural_Photo-Finishing_Supplementary_Information.pdf">supplement</a>
                  <p></p>
                  <p>An end-to-end differentiable pipeline for rendering sRGB images from raw inputs controlled by
                    meaningful parameters.
                  </p>
                </td>
              </tr>

              <!-- hsdepth -->
              <tr onmouseout="hsdepth_stop()" onmouseover="hsdepth_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='hsdepth_image'><img src='handshake_depth/after.jpg' width="100%"></div>
                    <img src='handshake_depth/before.jpg' width="100%">
                  </div>
                  <script type="text/javascript">
                    function hsdepth_start() {
                      document.getElementById('hsdepth_image').style.opacity = "1";
                    }
                    function hsdepth_stop() {
                      document.getElementById('hsdepth_image').style.opacity = "0";
                    }
                    hsdepth_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://light.princeton.edu/publication/hndr/">
                    <papertitle>Handheld Multi-Frame Neural Depth Refinement</papertitle>
                  </a>
                  <br>
                  <a href="https://ilyac.info/">Ilya Chugunov</a>,
                  <a href="https://www.alexyuxuanzhang.com/">Yuxuan Zhang</a>,
                  <strong><span style="font-size: 14px">Zhihao Xia</span></strong>,
                  <a href="https://ceciliavision.github.io/">Xuaner Zhang</a>,
                  <a href="https://people.csail.mit.edu/jiawen/">Jiawen Chen</a>,
                  <a href="https://www.cs.princeton.edu/~fheide/">Felix Heide</a>,
                  <br>
                  <em>CVPR</em>, 2022 &nbsp <font color=red><strong>(Oral Presentation)</strong></font> <br>
                  <a href="https://light.princeton.edu/publication/hndr/">project page</a> /
                  <a href="https://arxiv.org/abs/2111.13738">arxiv</a> /
                  <a href="https://github.com/princeton-computational-imaging/HNDR">code</a>
                  <p></p>
                  <p>High-fidelty depth recovery for tabletop objects from a single smartphone shot.
                  </p>
                </td>
              </tr>

              <!-- dfnc -->
              <tr onmouseout="dfnc_stop()" onmouseover="dfnc_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='dfnc_image'><img src='dfnc/after.jpg'></div>
                    <img src='dfnc/before.jpg'>
                  </div>
                  <script type="text/javascript">
                    function dfnc_start() {
                      document.getElementById('dfnc_image').style.opacity = "1";
                    }
                    function dfnc_stop() {
                      document.getElementById('dfnc_image').style.opacity = "0";
                    }
                    dfnc_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/pdf/2012.06125.pdf">
                    <papertitle>A Dark Flash Normal Camera</papertitle>
                  </a>
                  <br>
                  <strong><span style="font-size: 14px">Zhihao Xia</span></strong>,
                  <a href="https://scholar.google.com/citations?user=c4XKzcIAAAAJ&hl">Jason Lawrence</a>,
                  <a href="http://www.cs.cmu.edu/~sachar/">Supreeth Achar</a>
                  <br>
                  <em>ICCV</em>, 2021 <br>
                  <a href="https://darkflashnormalpaper.github.io/">project page</a> /
                  <a href="https://arxiv.org/abs/2012.06125">arxiv</a> /
                  <a href="https://youtu.be/RboGUBqYQec">video</a>
                  <p></p>
                  <p>Face surface normal and reflectance estimation under uncontrolled visible
                    lighting with a "dark flash image". </p>
                </td>
              </tr>


              <!-- deepFnF -->
              <tr onmouseout="deepFnF_stop()" onmouseover="deepFnF_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='deepFnF_image'><img src='deepfnf/dnz.jpg'></div>
                    <img src='deepfnf/nza.jpg'>
                  </div>
                  <script type="text/javascript">
                    function deepFnF_start() {
                      document.getElementById('deepFnF_image').style.opacity = "1";
                    }
                    function deepFnF_stop() {
                      document.getElementById('deepFnF_image').style.opacity = "0";
                    }
                    deepFnF_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/pdf/2012.05116.pdf">
                    <papertitle>Deep Denoising of Flash and No-Flash Pairs for Photography in Low-Light Environments
                    </papertitle>
                  </a>
                  <br>
                  <strong><span style="font-size: 14px">Zhihao Xia</span></strong>,
                  <a href="http://mgharbi.com/">Michaël Gharbi</a>,
                  <a href="https://fperazzi.github.io/">Federico Perazzi</a>,
                  <a href="https://www.kalyans.org/">Kalyan Sunkavalli</a>,
                  <a href="https://projects.ayanc.org/">Ayan Chakrabarti</a>
                  <br>
                  <em>CVPR</em>, 2021 <br>
                  <a href="https://likesum.github.io/deepfnf/">project page</a> /
                  <a href="https://arxiv.org/abs/2012.05116">arxiv</a> /
                  <a href="https://github.com/likesum/deepFnF/">code</a> /
                  <a href="https://youtu.be/2n4eY66JO5s">video</a>
                  <p></p>
                  <p>A neural network-based method to denoise pairs of images taken in quick succession in low-light
                    environments, with and without a flash.</p>
                </td>
              </tr>


              <!-- burstBPN -->
              <tr onmouseout="burstBPN_stop()" onmouseover="burstBPN_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='burstBPN_image'><img src='bpn/after.jpg'></div>
                    <img src='bpn/before.jpg'>
                  </div>
                  <script type="text/javascript">
                    function burstBPN_start() {
                      document.getElementById('burstBPN_image').style.opacity = "1";
                    }
                    function burstBPN_stop() {
                      document.getElementById('burstBPN_image').style.opacity = "0";
                    }
                    burstBPN_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/pdf/1912.04421.pdf">
                    <papertitle>Basis Prediction Networks for Effective Burst Denoising with Large Kernels</papertitle>
                  </a>
                  <br>
                  <strong><span style="font-size: 14px">Zhihao Xia</span></strong>,
                  <a href="https://fperazzi.github.io/">Federico Perazzi</a>,
                  <a href="http://mgharbi.com/">Michaël Gharbi</a>,
                  <a href="https://www.kalyans.org/">Kalyan Sunkavalli</a>,
                  <a href="https://projects.ayanc.org/">Ayan Chakrabarti</a> <br>
                  <em>CVPR</em>, 2020 <br>
                  <a href="https://likesum.github.io/bpn/">project page</a> /
                  <a href="https://arxiv.org/abs/1912.04421">arxiv</a> /
                  <a href="https://github.com/likesum/bpn/">code</a> /
                  <a href="https://youtu.be/jFubeqxJO6U">video</a>
                  <p></p>
                  <p>A basis prediction network that predicts global basis kernels and
                    corresponding per-pixel coefficients for burst denoising.</p>
                </td>
              </tr>


              <!-- prdepth -->
              <tr onmouseout="prdepth_stop()" onmouseover="prdepth_start()" bgcolor="#ffffd0">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='prdepth_image'><img src='prdepth/after.jpg'></div>
                    <img src='prdepth/before.jpg'>
                  </div>
                  <script type="text/javascript">
                    function prdepth_start() {
                      document.getElementById('prdepth_image').style.opacity = "1";
                    }
                    function prdepth_stop() {
                      document.getElementById('prdepth_image').style.opacity = "0";
                    }
                    prdepth_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/pdf/1906.05739.pdf">
                    <papertitle>Generating and Exploiting Probabilistic Monocular Depth Estimates</papertitle>
                  </a>
                  <br>
                  <strong><span style="font-size: 14px">Zhihao Xia</span></strong>,
                  Patrick Sullivan,
                  <a href="https://projects.ayanc.org/">Ayan Chakrabarti</a>
                  <br>
                  <em>CVPR</em>, 2020 &nbsp <font color=red><strong>(Oral Presentation)</strong></font> <br>
                  <a href="https://projects.ayanc.org/prdepth/">project page</a> /
                  <a href="https://arxiv.org/abs/1906.05739">arxiv</a> /
                  <a href="https://github.com/likesum/prdepth/">code</a> /
                  <a href="https://youtu.be/lNw9326KSlU">video</a>
                  <p></p>
                  <p>A common model that is only trained once for a variety of monocular depth applications.</p>
                </td>
              </tr>


              <!-- Intstatnn -->
              <tr onmouseout="intstatnn_stop()" onmouseover="intstatnn_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='intstatnn_image'><img src='intstatnn/intstatnn_after.jpg'></div>
                    <img src='intstatnn/intstatnn_before.jpg'>
                  </div>
                  <script type="text/javascript">
                    function intstatnn_start() {
                      document.getElementById('intstatnn_image').style.opacity = "1";
                    }
                    function intstatnn_stop() {
                      document.getElementById('intstatnn_image').style.opacity = "0";
                    }
                    intstatnn_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/pdf/1806.05229.pdf">
                    <papertitle>Identifying Recurring Patterns with Deep Neural Networks for Natural Image Denoising
                    </papertitle>
                  </a>
                  <br>
                  <strong><span style="font-size: 14px">Zhihao Xia</span></strong>,
                  <a href="https://projects.ayanc.org/">Ayan Chakrabarti</a>
                  <br>
                  <em>WACV</em>, 2020 <br>
                  <a href="https://projects.ayanc.org/rpcnn/">project page</a> /
                  <a href="https://arxiv.org/abs/1806.05229">arxiv</a> /
                  <a href="https://github.com/ayanc/rpcnn/">code</a>
                  <p></p>
                  <p>Image denoising by using DNN to exploit self-similarity in natural images.</p>
                </td>
              </tr>


              <!-- tiewig -->
              <tr onmouseout="tiewig_stop()" onmouseover="tiewig_start()" bgcolor="#ffffd0">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='tiewig_image'><img src='tiewig/after.jpg'></div>
                    <img src='tiewig/before.jpg'>
                  </div>
                  <script type="text/javascript">
                    function tiewig_start() {
                      document.getElementById('tiewig_image').style.opacity = "1";
                    }
                    function tiewig_stop() {
                      document.getElementById('tiewig_image').style.opacity = "0";
                    }
                    tiewig_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/pdf/1906.05775.pdf">
                    <papertitle>Training Image Estimators without Image Ground-Truth</papertitle>
                  </a>
                  <br>
                  <strong><span style="font-size: 14px">Zhihao Xia</span></strong>,
                  <a href="https://projects.ayanc.org/">Ayan Chakrabarti</a>
                  <br>
                  <em>NeurIPS</em>, 2019 &nbsp <font color=#FF8080><strong>(Spotlight)</strong></font> <br>
                  <a href="https://projects.ayanc.org/unsupimg/">project page</a> /
                  <a href="https://arxiv.org/abs/1906.05775">arxiv</a> /
                  <a href="https://github.com/likesum/unsupimg/">code</a>
                  <p></p>
                  <p>Unsupervised framework for training image estimation networks from only degraded or
                    partial measurements.
                  </p>
                </td>
              </tr>

              <!-- deepPolyA -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='deepPolyA/deepPolyA.jpg' width="100%" height="55%">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://drive.google.com/file/d/1IXxQ3yYqdY418GKUMsaELJqqZTxDohTt/view?usp=sharing">
                    <papertitle>DeeReCT-PolyA: a robust and generic deep learning method for PAS identification
                    </papertitle>
                  </a>
                  <br>
                  <strong><span style="font-size: 14px">Zhihao Xia</span></strong>,
                  <a href="https://liyu95.com/">Yu Li</a>,
                  Bin Zhang,
                  Zhongxiao Li,
                  Yuhui Hu,
                  <a href="http://www.sustc.edu.cn/en/biology_04/f/Chen_Wei">Wei Chen</a>,
                  <a href="https://sfb.kaust.edu.sa/Pages/Gao.aspx">Xin Gao</a>
                  <br>
                  <em>Bioinformatics</em>, 2018 <br>
                  <a
                    href="https://drive.google.com/file/d/1ez3Eoppbf-YdPxe8Yxb6OeYrEzYo9Nhv/view?usp=sharing">supplement</a>
                  /
                  <a href="https://github.com/likesum/DeeReCT-PolyA/">code</a>
                  <p></p>
                  <p>A robust deep learning model for the identification of polyadenylation signal - a critical factor
                    for gene expression.</p>
                </td>
              </tr>


              <!-- Multiple Scattering -->
              <tr onmouseout="mulScatter_stop()" onmouseover="mulScatter_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='mulScatter_image'><img src='multipleScattering/after.jpg'></div>
                    <img src='multipleScattering/before.jpg'>
                  </div>
                  <script type="text/javascript">
                    function mulScatter_start() {
                      document.getElementById('mulScatter_image').style.opacity = "1";
                    }
                    function mulScatter_stop() {
                      document.getElementById('mulScatter_image').style.opacity = "0";
                    }
                    intstatnn_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://www.osapublishing.org/viewmedia.cfm?uri=oe-26-11-14678&seq=0">
                    <papertitle>Efficient and accurate inversion of multiple scattering with deep learning</papertitle>
                  </a>
                  <br>
                  <a href="https://sunyumark.github.io/">Yu Sun</a>,
                  <strong><span style="font-size: 14px">Zhihao Xia</span></strong>,
                  <a href="https://cigroup.wustl.edu/ulugbek-s-kamilov/">Ulugbek S. Kamilov</a>
                  <br>
                  <em>Optics Express</em>, 2018 <br>
                  <p></p>
                  <p>A deep convolutional neural network for image reconstruction under multiple light
                    scattering.</p>
                </td>
              </tr>

            </tbody>
          </table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td>
                  <heading>Teaching</heading>
                </td>
              </tr>
            </tbody>
          </table>
          <table width="100%" align="center" border="0" cellpadding="20">
            <tbody>
              <tr>
                <td width="25%"><img src="ComputerVision2018_TA/cv.jpg" alt="CV2018" width="160" height="160"></td>
                <td width="75%" valign="center">
                  <p>
                    <a href="https://www.cse.wustl.edu/~ayan/courses/cse559a/">
                      <papertitle>CSE559A: Computer Vision - Fall 2018 </papertitle>
                    </a>
                    <br>Teaching Assistant
                    <br><br>
                  </p>
                </td>
              </tr>
          </table>
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tr>
              <td>
                <br>
                <p align="right">
                  <font size="2">
                    <a href="https://jonbarron.info/">This webpage is cool</a>
                  </font>
                </p>
              </td>
            </tr>
    </tbody>
  </table>

</html>
